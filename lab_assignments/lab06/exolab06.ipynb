{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExoStat Lab 06: Principal Components Analysis and Stellar Activity\n",
    "\n",
    "**Administrative details:**\n",
    "\n",
    "- This Lab will be turned in for credit.\n",
    "\n",
    "- Some questions of this lab are the same as the Practice 06 questions found on the main [YData website](http://ydata123.org/sp19/).  \n",
    "\n",
    "- Collaborating on the ExoStat Labs is encouraged. If you get stuck for a while on a question, feel free to ask a neighbor or come to the instructor's or TF's office hours for additional help. (Explaining things is beneficial, too -- the best way to solidify your knowledge of a subject is to explain it.) Please don't just share answers, though.\n",
    "\n",
    "This term we will be using Piazza for class discussion. Find our class page [here](https://piazza.com/yale/spring2019/sds170/home)\n",
    "\n",
    "You can read more about course policies on our [canvas site](https://canvas.yale.edu).\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "This assignment is due Monday, March 4th at 11:59 P.M. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on [Canvas](https://canvas.yale.edu)).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged. Refer to the policies page to learn more about how to learn cooperatively.\n",
    "\n",
    "#### Today's ExoStat Lab\n",
    "\n",
    "1.  Hypothesis testing\n",
    "\n",
    "2. Principal Components Analysis (PCA)\n",
    "\n",
    "3.  Stellar spectra\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Submit your assignment both as a .pdf and .ipynb (Jupyter notebook) in Canvas.  \n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "To produce the .ipynb, please do the following:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"Notebook (.ipynb)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell, but please don't change it.\n",
    "\n",
    "# These lines import the Numpy and Datascience modules.\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "\n",
    "# These lines do some fancy plotting magic\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "from matplotlib import patches\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Hypothesis testing\n",
    "\n",
    "**For those of you not in the main YData course, it is strongly suggested that you read through [Chapter 11](https://www.inferentialthinking.com/chapters/11/Testing_Hypotheses.html) of the textbook to learn about hypothesis testing.**\n",
    "\n",
    "### What is the Therapeutic Touch\n",
    "\n",
    "The Therapeutic Touch (TT) is the idea that everyone can feel the Human Energy Field (HEF) around individuals. Certain practictioners claim they have the ability to feel the HEF and can massage it in order to promote health and relaxation in individuals. Those who practice TT have described different people's HEFs as \"warm as Jell-O\" and \"tactile as taffy\". \n",
    "\n",
    "TT was a popular technique used throughout the 20th century that was toted to be a great way to bring balance to a person's health. \n",
    "\n",
    "### Emily Rosa\n",
    "\n",
    "Emily Rosa was a 4th grade student who had wide exposure to the world of TT due to her parents. Her parents were both medical practitioners and skeptics of the idea of TT. \n",
    "\n",
    "For her 4th grade science fair project, Emily decided to test whether or not TT practitioners could truly interact with a person's HEF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1:** How would you set up an experiment to test this?  Feel free to discuss with your neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emily's Experiment\n",
    "\n",
    "Emily's experiment was clean, simple, and effective. Due to her parents' occupations in the medical field, she had wide access to people who claimed to be TT practitioners. \n",
    "\n",
    "Emily took 21 TT practitioners and used them for her science experiment. She would take a TT practitioner and ask them to extend their hands through a screen (through which they can't see). Emily would be on the other side and would flip a coin. Depending on how the coin landed, she would put out either her left hand or her right hand. The TT practitioner would then have to correctly answer which hand Emily put out. Overall, through 210 samples, the practitioner picked the correct hand 44% of the time. \n",
    "\n",
    "Emily's main goal here was to test whether or not the TT practicioners' guesses were random, like the flip of a coin. In most medical experiments, this is the norm. We want to test whether or not the treatment has an effect, *not* whether or not the treatment actually works. \n",
    "\n",
    "We will now begin to formulate this experiment in terms of the terminology we learned in the YData course.  Read about hypothesis testing in [Chapter 11](https://www.inferentialthinking.com/chapters/11/Testing_Hypotheses.html) and [Chapter 12](https://www.inferentialthinking.com/chapters/12/Comparing_Two_Samples.html) of the textbook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2**: What are the null and alternative hypothesis for Emily's experiment? Discuss with students around you to come to a conclusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "for_assignment_type": "student"
   },
   "source": [
    "**Your Answer Here:**\n",
    "\n",
    "Null Hypothesis: \n",
    "\n",
    "Alternative Hypothesis: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Remember that the practitioner got the correct answer 44% of the time. According to the null hypothesis, on average, what proportion of times do we expect the practitioner to guess the correct hand? Make sure your answer is between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_correct = ...\n",
    "expected_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal now is to see if our deviation from this expected proportion of correct answers is due to something other than chance. \n",
    "\n",
    "**Question 4:** What is a valid test statistic we can use to test our model? Assign `valid_ts` to a list of integers representing the following options: \n",
    "\n",
    "1. The difference of the expected percent correct and the actual percent correct\n",
    "2. The absolute difference of the expected percent correct and the actual percent correct\n",
    "3. The sum of the expected percent correct and the actual percent correct\n",
    "\n",
    "There may be more than one correct answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ts = ...\n",
    "valid_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Define the function `test_statistic` which takes in an expected proportion and an actual proportion, and returns the value of the test statistic chosen above. Assume that you are taking in proportions, but you want to return your answer as a percentage. \n",
    "\n",
    "*Hint:* Remember we are asking for a **percentage**, not a proportion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_statistic(expected_prop, actual_prop):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Use your newly defined function to calculate the observed test statistic from Emily's experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_test_statistic = ...\n",
    "observed_test_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this test statistic likely if the null hypothesis was true? Or is the deviation from the expected proportion due to something other than chance?**\n",
    "\n",
    "In order to answer this question, we can simulate the experiment as though the null hypothesis was true, and calculate the test statistic per each simulation.\n",
    "\n",
    "**Question 7:** To begin simulating, we should start by creating an array which has two items in it. The first item should be the proportion of times, assuming the null model is true, a TT practictioner picks the correct hand. The second item should be the proportion of times, under the same assumption, that the TT practicioner picks the incorrect hand. Assign `model_proportions` to this array. After this, simulate, using the `sample_proportions` function, Emily running through this experiment 210 times (as done in real life), and assign the proportion of correct answers to `simulation_proportion`. Lastly, define `one_test_statistic` to the test statistic of this one simulation. \n",
    "\n",
    "*Hint:* `sample_proportions` usage can be found here: [here](http://data8.org/sp18/python-reference.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_proportions = ...\n",
    "simulation_proportion = ...\n",
    "one_test_statistic = ...\n",
    "one_test_statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** Let's now see what the distribution of test statistics is actually like under our fully specified model. Assign `simulated_test_statistics` to an array of 1000 test statistics that you simulated assuming the null hypothesis is true. \n",
    "\n",
    "*Hint:* This should follow the same pattern as normal simulations, in combination with the code you did in the previous problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "for_assignment_type": "student",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_repetitions = 1000\n",
    "num_guesses = 210\n",
    "\n",
    "simulated_test_statistics = ...\n",
    "\n",
    "for ... in ...:\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the distribution of the simulated test statistics under the null, and visually compare how the observed test statistic lies against the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table().with_column('Simulated Test Statistics', simulated_test_statistics)\n",
    "t.hist()\n",
    "plt.scatter(observed_test_statistic, 0, color='red', s=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a visual argument as to whether or not we believe the observed test statistic is likely to occur under the null, or we can use the definition of p-values to help us make a more formal argument. \n",
    "\n",
    "**Question 9:** Assign `p_value` to the integer corresponding to the correct definition of what a p-value really is. \n",
    "\n",
    "1. The chance, under the null hypothesis, that the test statistic is equal to the value that was observed\n",
    "2. The chance, under the null hypothesis, that the test statistic is equal to the value that was observed or is even further in the direction of the alternative\n",
    "3. The chance, under the alternative hypothesis, that the test statistic is equal to the value that was observed or is even further in the direction of the null \n",
    "4. The number of times, under the null hypothesis, that the test statistic is equal to the value that was observed or is even further in the direction of the alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = ...\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10:** Using the definition above, calculate the p-value of Emily's observed value in this experiment. \n",
    "\n",
    "*Hint:* If our test statistic is further in the direction of the alternative, will larger value or a smaller value? \n",
    "\n",
    "*Hint:* [This section](https://www.inferentialthinking.com/chapters/11/3/decisions-and-uncertainty) of the textbook contains an example of calculating an empirical p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emily_p_val = ...\n",
    "emily_p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our p-value is less than or equal to .05, then this is in favor of our alternative and we reject the null hypothesis. Otherwise, we do not have enough evidence against our null hypothesis. Note that this does **not** say we side in favor with the null hypothesis and accept it, but rather, that we just fail to reject it. \n",
    "\n",
    "This should help you make your own conclusions about Emily Rosa's experiment. \n",
    "\n",
    "Therapeutic touch fell out of use after this experiment, which was eventually accepted into one of the premier medical journals. TT practitioners hit back and accused Emily and her family of tampering with the results, while some claimed that Emily's bad spiritual mood towards therapeutic touch made it difficult to read her HEF. Whatever it may be, Emily's experiment is a classic example about how anyone, with the right resources, can test anything they want!\n",
    "\n",
    "Think about the following questions: \n",
    "\n",
    "1. Do we reject the null hypothesis, or fail to reject it? \n",
    "2. What does this mean in terms of Emily's experiment? Do the TT practitioners' answers follow an even chance model or is there something else at play? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Put your reponse here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Components Analysis (PCA)\n",
    "\n",
    "[Principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is popular statistical method, and is widely used in other fields as well.  There are many uses for PCA, such as for reducing the dimension of data or defining a set of variables from the data that are uncorrelated.  For this lab, we are going to use PCA as a way of investigating, understanding, and visualizing the variability of our data.\n",
    "\n",
    "To begin, we are going to learn some of the basics by exploring some problems found in [this guide](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html).\n",
    "\n",
    "\n",
    "First let's install the module with the PCA functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of details about PCA that are beyond the scope of our course.  But let's look at a few aspects of what goes into it.\n",
    "\n",
    "Below is code to create a simulated dataset for us.  The `rng` fixes the random number so that we all will use the same randomly generated data.  The dataset is created and then plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we can run the PCA on our data `X`.  Below is the code for it.  Notice that first we define some properties of the PCA using the `PCA()` option.  In this case, all we are doing is specifying the number of principal components.  We set this to `n_components=2` because we only have two dimensions to our data.  Sometimes you will have many dimensions, but will only specify a small number of components like 2 - 10 (which will often contain most of the variability in the data).\n",
    "\n",
    "After specifying the details of the PCA, we run it on our data using `pca.fit(X)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the PCA is stored in `pca`, which we can look at.  For example, we can look at the two principal components by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the two principal component vectors\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first principal component, PC1, is the vector [-.94445, -0.32862557].  This means that the direction of this vector explains the most variability in the data.  We are going to be plotting this vector below so you can get a sense of what that means.\n",
    "\n",
    "Another output of `pca` is `pca.explained_variance_`.  This specifies how much variability is explained by each of the two PCs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that variability PC1 acounts for .7625 and PC2 accounts for 0.0184.  Since there are only two dimensions to our data, the total variability in the data is the sum of these two values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total variability\n",
    "sum(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the total variability by adding the variance of the two columns of `X` together as is done in the cell below.  The factor `200/199` is to account for a difference in the way variance is calculated with `np.var` and the PCA function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.var(X[:,0])+np.var(X[:,1]))*200/199"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at the raw variance, often people are interested in the proportion of the variability accounted for by each component.  This information is stored in `pca.explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is simply the ratio of the variability of each component divided by the total variability.  We could calculate it manually below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_/sum(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean vector is also stored, which is simply the mean of the different columns of `X` (as seen in the second cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.mean(X[:,0]),np.mean(X[:,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the PCs!  Below is code for plotting the data and then adding the principal component vectors as arrows.  PC1 is the longest vector and indicates the direction of maximum variability.  The second arrow is for PC2 and is perpendicular to the PC1 vector.  Both of the vectors are drawn from the mean, `pca.mean_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color = \"blue\")\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3* np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1.** Now it is your turn!  Below is a new dataset.  Run the cell below to see what it looks like.  Where do you think PC1 is going to be?  Put your answer in the indicated cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to get the new dataset.\n",
    "A = [[-1,-1], \n",
    "    [.5,1]]\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(A, rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Add you response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2.** Run a PCA on the new data.  What are the values of the PC1 vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to run PCA here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values of PC1\n",
    "pc1 = ...\n",
    "pc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3.** Now produce a plot like the one above with the arrows.  You should be able to simply copy the plotting code from above and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4.**  What percentage of variability is explained by PC2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to look at the `exams.txt` dataset.  These are the scores of 88 students in five different math areas:  vectors, mechanics, algebra, analysis, statistics.  Read in the data below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams0 = Table.read_table(\"exams.txt\", sep = \"\\s+\", header = None, \n",
    "                         names = [\"Vectors\",\"Mechanics\",\"Algebra\",\"Analysis\",\"Stat\"])\n",
    "exams0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the PCA functions, we need the dataset to be an array.  The cell below converts our table into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams = np.zeros((exams0.num_rows, exams0.num_columns))\n",
    "for i in np.arange(exams0.num_rows):\n",
    "    exams[i] = exams0.values[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5.**  Run a PCA on the `exams` data and print out the principal components.  Since there are five columns in our dataset, set the `n_components` to be 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6.**  Next we are going to produce a plot like was discussed during the lecture portion of the class, which is called a [biplot](https://en.wikipedia.org/wiki/Biplot).  The code is provided for you below, for this question you just need to run the code.  The points that are plotted are the 88 students projected onto the first two principal component vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## project data into PC space\n",
    "dat = exams\n",
    "labels = [\"Vectors\",\"Mechanics\",\"Algebra\",\"Analysis\",\"Stat\"]\n",
    "# 0,1 denote PC1 and PC2; change values for other PCs\n",
    "xvector = pca.components_[0] # see 'prcomp(my_data)$rotation' in R\n",
    "yvector = pca.components_[1]\n",
    "\n",
    "xs = pca.transform(dat)[:,0] # see 'prcomp(my_data)$x' in R\n",
    "ys = pca.transform(dat)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize projections\n",
    "for i in range(len(xvector)):\n",
    "# arrows project features (ie columns from csv) as vectors onto PC axes\n",
    "    plt.arrow(0, 0, xvector[i]*max(xs), yvector[i]*max(ys),\n",
    "              color='r', width=0.0005, head_width=0.0025)\n",
    "    plt.text(xvector[i]*max(xs)*1.2, yvector[i]*max(ys)*1.2,\n",
    "             labels[i], color='r')\n",
    "\n",
    "for i in range(len(xs)):\n",
    "# circles project documents (ie rows from csv) as points onto PC axes\n",
    "    plt.plot(xs[i], ys[i], 'bo')\n",
    "    plt.text(xs[i]*1.2, ys[i]*1.2, i, color='b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7.**  Now we want to interpret the biplot from above.  As we did during lecture, find a few students that are well-separated in the PC1 direction and explain what it is about their scores that seems to make them different.  That is, what seems to be leading to the variability in this direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.8.**  Same question as above, except for the PC2 direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stellar activity\n",
    "\n",
    "In this section, we are going to look at some simulated stellar spectra.  These spectra were generated from [SOAP 2.0](https://arxiv.org/abs/1409.3594), which is tool for simulating stellar spectra with different types of stellar activity such as spots.\n",
    "\n",
    "Next week we are going to consider running a PCA on these data, but today we are only going to make a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the wavelengths\n",
    "wavelength = Table.read_table(\"spot_1_percent_wave.txt\", header = None, names = \"Wave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a header for the spectra data\n",
    "spec_header = make_array()\n",
    "for i in np.arange(wavelength.num_rows):\n",
    "    spec_header = np.append(spec_header, str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the spectra data\n",
    "spec = Table.read_table(\"spot_1_percent.txt\",sep = \"\\s+\", header = None, names = spec_header)\n",
    "spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1.** How many rows and columns does `spec` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_rows = ...\n",
    "spec_columns = ...\n",
    "print(\"Rows: \", spec_rows)\n",
    "print(\"Columns: \", spec_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2.** Make a plot with `wavelength` on the horizontal axis and the first row of `spec` on the vertical axis.  The horizontal axis label can be `Wavelength` and the vertical axis label can be `Intensity`. Hint:  You may find `spec.values[0]` helpful in producing the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, we will look at these data again next week and run a PCA on them in order to investigate stellar activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Project Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final project is an opportunity for you to explore, in more detail, a question of interest related to exoplanets and data science.  It is expected that the topic of the question will be related to exoplanets and the project will include an analysis using data science methods.  \n",
    "\n",
    "The final project will culminate in a 5 - 10 page written report introducing your question, describing the methodology you employ to answer the question, a discussion of the results, and finally the conclusions you can draw from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1.** It is time to start thinking about ideas for your final project.  For this question, describe an idea (or several) that you are intersted in pursuing for your final project.  Explain the question or topic you plan to explore along with the type or source of data you would need for addressing your question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**: Once you're finished, follow the instructions at the top of this notebook to save as a .pdf and .ipynb. Then submit the two files through Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
