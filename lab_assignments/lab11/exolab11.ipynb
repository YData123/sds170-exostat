{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExoStat Lab 11:  Clustering and Classification\n",
    "\n",
    "**Administrative details:**\n",
    "\n",
    "- This Lab will be turned in for credit.\n",
    "\n",
    "- Some questions of this lab are variations of lecture notes and demos found on the main [YData website](http://ydata123.org/sp19/).  \n",
    "\n",
    "- Collaborating on the ExoStat Labs is encouraged. If you get stuck for a while on a question, feel free to ask a neighbor or come to the instructor's or TF's office hours for additional help. (Explaining things is beneficial, too -- the best way to solidify your knowledge of a subject is to explain it.) Please don't just share answers, though.\n",
    "\n",
    "This term we will be using Piazza for class discussion. Find our class page [here](https://piazza.com/yale/spring2019/sds170/home)\n",
    "\n",
    "You can read more about course policies on our [canvas site](https://canvas.yale.edu).\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "This assignment is due Monday, April 29th at 11:59 P.M. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on [Canvas](https://canvas.yale.edu)).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged. Refer to the policies page to learn more about how to learn cooperatively.\n",
    "\n",
    "#### Today's ExoStat Lab\n",
    "\n",
    "1. Clustering:  See lecture notes\n",
    "\n",
    "2. Classification:\n",
    "See lecture notes or textbook pages [here](https://www.inferentialthinking.com/chapters/17/Classification.html)\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Submit your assignment both as a .pdf and .ipynb (Jupyter notebook) in Canvas.  \n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "To produce the .ipynb, please do the following:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"Notebook (.ipynb)\"\n",
    "\n",
    "Let's begin by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell, but please don't change it.\n",
    "\n",
    "# These lines import the Numpy and Datascience modules.\n",
    "import numpy as np\n",
    "from datascience import *\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# These lines do some fancy plotting magic\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "from matplotlib import patches\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed and used during the lecture, we are going to use a simulated dataset to illustrate clustering and classification.  The cell below reads in the simulated dataset, `data.txt`.  This is the \"unlabeled\" data, which we will use for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = Table.read_table(\"data.txt\", delimiter = \" \", names = [\"x\",\"y\"])\n",
    "sim.scatter(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels/classes for `sim` can be loaded using the cell below.  Run the cell below to load the labels, `data_labels.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_labels = Table.read_table(\"data_labels.txt\", delimiter = \" \", names = [\"labels\"]).column(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a scatterplot of `sim`, but colors the points according to their label.  Notice that there are five different labels/classes.  We will use this dataset for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.with_columns(\"Class\", sim_labels).scatter(0,1,colors = \"Class\") \n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-6,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed the k-means clustering method during lecture.  To implement it, we will use `KMeans` in the Python module `sklearn.cluster`. (This was loaded in the setup cell above.)  You can learn more about `KMeans` [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
    "\n",
    "It turns out to use `KMeans`, the data format needs to be an array.  The code below takes the columns of our `sim` table and combines them as a 2D array.  Use `sim_km` in `KMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_km = np.column_stack(make_array(sim.column(0), sim.column(1)))\n",
    "print(type(sim_km))\n",
    "print(sim_km.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first consider running this with two clusters (k = 2).  The code below specificies in `KMeans` that we would like k=`n_clusters=2`.  The `n_init` specifies that number of different starting means.  The `.fit(sim_km)` gets us the fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclust2 = 2\n",
    "sim_km2 = KMeans(n_clusters=nclust2, n_init = 10).fit(sim_km)\n",
    "sim_km2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `.cluster_centers_` specifies the centers of the two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_km2_centers = sim_km2.cluster_centers_\n",
    "sim_km2_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `.labels_` stores the cluster assignment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_km2_labels = sim_km2.labels_\n",
    "sim_km2_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below produces a plot of `sim` with the points colored according to the cluster assignments.  The centers of the clusters are indicated with an `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.with_columns(\"Cluster\", sim_km2_labels).scatter(0,1,colors = \"Cluster\") \n",
    "plt.scatter(sim_km2_centers[:,0],sim_km2_centers[:,1], marker = \"x\", s = 100)\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-6,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desiring to compare the cluster models under different values of `n_clusters`, sometimes \"elbow plots\" are employed.  An elbow plot displays the *within cluster* variability against varying numbers of clusters.  (See the lecture notes for additional discussion.)  In `KMeans`, the `.inertia_` gives the within cluster variability for the specified value of `n_clusters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_km2.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the basics of k-means clustering down, let's try this out!  If you are unsure about the questions below, refer back to the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1.**  Make a scatterplot of `sim` and color the points according to the k-means result with k = 5.  Be sure to plot the cluster centers as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2.**  In the question above with k = 5, does this look like the true class assignments?  If it does not match the `sim_labels`, is there any chance that k-means clustering would be able to find the `sim_labels`?  Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Add your response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3.**  Make an elbow plot and determine the \"best\" number of clusters.  Be sure to label your axes.  You will need to think about the appropriate range for the number of clusters...make sure to have at least 1!\n",
    "\n",
    "Then, make a scatterplot of `sim` and color the points according to the k-means with your selected number of clusters.  Be sure to plot the cluster centers as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Elbow plot\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot with \"best\" k\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try k-means clustering for exoplanets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a subset of the confirmed exoplanet data, `confirmed_planets.csv`, and was collected from the [NASA Exoplanet Archive](https://exoplanetarchive.ipac.caltech.edu).  \n",
    "\n",
    "Let's start with considering the planet mass and planet radius.  See [Chen and Kipping (2017)](https://iopscience.iop.org/article/10.3847/1538-4357/834/1/17/pdf) for a clustering approach using advanced statistical methods.  Figure 3 presents a possible interpretation of the clusters they found.\n",
    "\n",
    "\n",
    "Below we read in the data, remove the `nan`, and then convert to Earth units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "exoplanets = Table.read_table(\"confirmed_planets.csv\", skiprows = 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove nans\n",
    "def remove_nan(x):\n",
    "    if any(np.isnan(x)):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = exoplanets.column(\"pl_bmassj\")*317.8\n",
    "radius = exoplanets.column(\"pl_radj\")*11.2\n",
    "data1 = Table().with_columns(\"mass\", mass, \"radius\", radius)\n",
    "nans1 = data1.apply(remove_nan)\n",
    "data1 = data1.where(nans1)\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3.**  Make a scatterplot of Radius vs. Mass on the log scale.  Be sure to label your axes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to turn the data into a 2D array to use in `KMeans`.  Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a 2D array\n",
    "data1_km = np.column_stack(make_array(data1.column(0), data1.column(1)))\n",
    "data1_km.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4.**  Try out k-means clustering for the radius and mass data.  Make plots of several different values for `k` on the log scale, with the colors corresponding to the assigned clusters.  How do they look?  Do the cluster assignments look like what you would expect?  Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5.**  Make an elbow plot and determine the \"best\" number of clusters.  Be sure to label your axes.  You will need to think about the appropriate range for the number of clusters.\n",
    "\n",
    "Then, make a scatterplot of `data1` and color the points according to the k-means with your selected number of clusters.  Be sure to plot the cluster centers as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Elbow plot\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot with \"best\" k\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6.**  Now you get to do some exploring!  Pick out two columns from the exoplanet data for which you would like to find clusters.  You should include the following:\n",
    "\n",
    "(i) A scatterplot of the two variables you are considering (be sure to label your axes).\n",
    "(ib) Explain why you selected these two variables\n",
    "\n",
    "(ii) Make an elbow plot to decide on the number of clusters you would like to consider\n",
    "\n",
    "(iii) Make a scatterplot of you data with the points colored according to the cluster assignments you found in part (ii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data table of your variables (you may need to remove nans)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Explain why you selected the two variables here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatterplot of data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an elbow plot and decide on the number of clusters\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit k-means model\n",
    "# Make a scatterplot of data with points colored according \n",
    "##  to the cluster assignment\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Classification\n",
    "\n",
    "Next we are going to move into classification.  While clustering is unsupervised (no labels or classes used), classification *does* use labels and is considered a supervised approach because of this.\n",
    "\n",
    "We are going to focus on k-nearest neighbers (kNN) classification, which you can (and are encouraged to) read about in the [textbook](https://www.inferentialthinking.com/chapters/17/Classification.html).\n",
    "\n",
    "First we will use our simulated dataset, `sim`, along with the assigned labels, `sim_labels`.  We are going to walk through the steps for building a classification model using the simulated data.  If anything is unclear, refer back to today's lecture notes or the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  the training dataset\n",
    "\n",
    "Below is our simulated dataset along with the labels.  Classification is different from clustering in many ways, but one major difference is that there *is* a correct classification we will be able to assess the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim2 = sim.with_columns(\"Class\", sim_labels)\n",
    "\n",
    "sim2.scatter(\"x\",\"y\", colors = \"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distance function\n",
    "\n",
    "Since we are using kNN classification, we will need to have a way to determine who the nearest neighbors are...this requires us to specify a distance function.  The function `distance` below calculates the distance between two points, and `row_distance` calculates the distance between two rows in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this\n",
    "def distance(pt1, pt2):\n",
    "    \"\"\"Return the distance between two points, represented as arrays\"\"\"\n",
    "    return np.sqrt(sum((pt1 - pt2)**2))\n",
    "\n",
    "def row_distance(row1, row2):\n",
    "    \"\"\"Return the distance between two numerical rows of a table\"\"\"\n",
    "    return distance(np.array(row1), np.array(row2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out to make sure we understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this to see the table format\n",
    "sim2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the attributes - what we want to use for the classification \n",
    "sim_attributes = sim2.drop(\"Class\")\n",
    "sim_attributes.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between two rows\n",
    "row_distance(sim_attributes.row(0), sim_attributes.row(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what row_distance is doing\n",
    "np.sqrt((-2.82951--3.0712)**2 + (.0903454-.110908)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe for finding the nearest neighbors\n",
    "\n",
    "Now that we have our distance defined, we want to use it for nearest neighbors classification.  This subsection defines a function for calculating the distance between an example point (of which we would like to know the class) and all the points in our training set (we will define the training and test set soon!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest neighbors procedure\n",
    "def distances(training, example):\n",
    "    \"\"\"Compute distance between example and every row in training.\n",
    "    Return training augmented with Distance column\"\"\"\n",
    "    distances = make_array()\n",
    "    attributes = training.drop('Class')\n",
    "    for row in attributes.rows:\n",
    "        distances = np.append(distances, row_distance(row, example))\n",
    "    return training.with_column('Distance', distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following example point and try to predict it's label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = sim_attributes.row(12)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we get the distances between `example` and the points in the dataset in ascending order.  That is, the first row is `example`'s nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances(sim2.exclude(12), example).sort('Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the list of points and distances, we need to determine the `k` closest points to determine our classification.  The function below returns the `k` nearest neighbors of some example point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(training, example, k):\n",
    "    \"\"\"Return a table of the k closest neighbors to example\"\"\"\n",
    "    return distances(training, example).sort('Distance').take(np.arange(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gives the 5 nearest neighbors to example\n",
    "closest(sim2.exclude(12), example, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `k` nearest neighbors can now be used to define the majority class (that is, the majority class among the `k` nearest neighbors).  The two functions below lead us to ultimately be able to classify an example point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class(topk):\n",
    "    \"\"\"Return the class with the highest count\"\"\"\n",
    "    return topk.group('Class').sort('count', descending=True).column(0).item(0)\n",
    "\n",
    "def classify(training, example, k):\n",
    "    \"Return the majority class among the k nearest neighbors of example\"\n",
    "    return majority_class(closest(training, example, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify our example point\n",
    "classify(sim2.exclude(12), example, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true class for the example point\n",
    "sim2.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try another example\n",
    "new_example = sim_attributes.row(100)\n",
    "classify(sim2.exclude(100), new_example, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim2.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Make a training and test set\n",
    "\n",
    "We now have an approach for building our kNN classification model.  As discussed in the lecture and in the textbook, we should separate our data into a *training* dataset and a *testing* dataset.  The training dataset is used for building our model, and the testing dataset is how we can assess the performance of our classifier.  Recall that the testing data are not used in building the model so we get a better assessment of the prediction accuracy.  \n",
    "\n",
    "Let's use half the observations for training and half for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observations\n",
    "sim2.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the order of the observations, then divide into \n",
    "## the training and testing sets\n",
    "shuffled = sim2.sample(with_replacement=False) # Randomly permute the rows\n",
    "training_set = shuffled.take(np.arange(125))\n",
    "test_set  = shuffled.take(np.arange(125,250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1.** Create two scatterplots:  one of the training dataset, and one of the testing dataset, both with points colored according to the true class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Build the model using the training set\n",
    "\n",
    "Next we need to build our classification model using the traning dataset.  We updated the functions used previously and added some new functions in the cell below so that they work for a table of values we would like to predict.  Read over and review the functions to make sure you understand what each one is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the distance between two points\n",
    "def distance(pt1, pt2):\n",
    "    return np.sqrt(np.sum((pt1 - pt2)**2))\n",
    "\n",
    "#Calculates distance between training set and a point p\n",
    "def all_dists(training, p):\n",
    "    attributes = training.drop('Class')\n",
    "    def dist_point_row(row):\n",
    "        return distance(np.array(row), p)\n",
    "    return attributes.apply(dist_point_row)\n",
    "\n",
    "#Creates a table with the distances between training and point p\n",
    "def table_with_distances(training, p):\n",
    "    return training.with_column('Distance', all_dists(training, p))\n",
    "\n",
    "#Finds the k nearest neighbors to p from training\n",
    "def closest(training, p, k):\n",
    "    with_dists = table_with_distances(training, p)\n",
    "    sorted_by_dist = with_dists.sort('Distance')\n",
    "    topk = sorted_by_dist.take(np.arange(k))\n",
    "    return topk\n",
    "\n",
    "#Finds the majority class among k nearest neighbors\n",
    "def majority_class(topk):\n",
    "    \"\"\"Return the class with the highest count\"\"\"\n",
    "    return topk.group('Class').sort('count', descending=True).column(0).item(0)\n",
    "\n",
    "#Classifies point p using k nearest neighbors of p\n",
    "def classify(training, p, k):\n",
    "    closestk = closest(training, p, k)\n",
    "    topkclasses = closestk.select('Class')\n",
    "    return majority_class(topkclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to create a plot that helps us to define the decision boundaries of the classification model.  We can do this by defining a grid across `x` and `y` and then getting the predicted labels for each grid point.  The next few cells produce this...make sure you understand what each cell is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifies each point in the grid using kNN\n",
    "def classify_grid(training, test, k):\n",
    "    c = make_array()\n",
    "    for i in range(test.num_rows):\n",
    "        # Run the classifier on the ith patient in the test set\n",
    "        c = np.append(c, classify(training, make_array(test.row(i)), k))   \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid\n",
    "x_array = make_array()\n",
    "y_array = make_array()\n",
    "for x in np.arange(-4, 4, 0.2):\n",
    "    for y in np.arange(-5, 4, 0.2):\n",
    "        x_array = np.append(x_array, x)\n",
    "        y_array = np.append(y_array, y)\n",
    "        \n",
    "test_grid = Table().with_columns(\n",
    "    'x', x_array,\n",
    "    'y', y_array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the classification labels\n",
    "c = classify_grid(training_set, test_grid, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class label-color table \n",
    "## (defined to match the class label colors previously used)\n",
    "color_table = Table().with_columns(\n",
    "    'Class', np.arange(1,6),\n",
    "    'Color', make_array('darkblue', 'gold', 'lightblue', 'green', 'red')\n",
    ")\n",
    "color_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of the predicted class lables for the test grid \n",
    "## colored according to the class label\n",
    "#The training points are added and colored to match the class labels\n",
    "\n",
    "test_grid = test_grid.with_column('Class', c)\n",
    "test_grid.scatter('x', 'y', colors='Class', alpha=0.4, s=30)\n",
    "\n",
    "training2 = training_set.join(\"Class\", color_table)\n",
    "plt.scatter(training2.column('x'), training2.column('y'), c=training2.column('Color'), edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2**  What does the decision boundary plot above tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Add response here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Evaluate the model using the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how well our model performs by predicting the labels of the test dataset and comparing the predicted labels to the true labels.  The function `evaluate_accuracy` estimates the *test error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(training, test, k):\n",
    "    \"\"\"Return the proportion of correctly classified examples \n",
    "    in the test set\"\"\"\n",
    "    test_attributes = test.drop('Class')\n",
    "    num_correct = 0\n",
    "    for i in np.arange(test.num_rows):\n",
    "        c = classify(training, test_attributes.row(i), k)\n",
    "        num_correct = num_correct + (c == test.column('Class').item(i))\n",
    "    return num_correct / test.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to see the test error rate for different values of `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_accuracy(training_set, test_set, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_accuracy(training_set, training_set, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_accuracy(training_set, training_set, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now seen how to fit a classification model.  In these next questions, you get to decide on your own classification model using the confirmed exoplanet data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3.**  Select two attributes from the confirmed exoplanet data along with classes for which you would like to try to build a kNN classification model.  Make a scatterplot of the attributes and color the points according to the classes that you have selected. Be sure to label the axes! \n",
    "\n",
    "A couple ideas for possible classes are `pl_letter` (indicating the order in which planets are discovered in a system) or `pl_discmethod` (the discovery method).  You will need to decide which attributes might be useful for predicting the classes you select!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4.** Divide the data into a training set and a testing set and make a scatterplot of each with the points colored according the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5.** Build your classification model using your training set, and create a plot of the decision boundary as was done above using the grid.  Add the training data points to the grid as well (colored to match the appropriate grid label colors).  You will likely need to adjust the functions used in the previous example to work in your new setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6.**  Use your test dataset to estimate the test error rates for at least three different values for `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission**: Once you're finished, follow the instructions at the top of this notebook to save as a .pdf and .ipynb. Then submit the two files through Canvas."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
